---
name: prompt_optimizer
router_kit: AIKit
description: Optimizing token usage and success rate of existing prompts.
metadata:
  skillport:
    category: ai
    tags: [agents, algorithms, artificial intelligence, automation, chatbots, cognitive services, deep learning, embeddings, frameworks, generative ai, inference, large language models, llm, machine learning, model fine-tuning, natural language processing, neural networks, nlp, openai, prompt engineering, prompt optimizer, rag, retrieval augmented generation, tools, vector databases, workflow automation]      - prompt-refinement
---

# âš¡ Prompt Optimizer

> Methodology for increasing prompt efficiency and reducing token cost.

---

*Prompt Optimizer v1.1 - Enhanced*

## ðŸ”„ Workflow

> **Source:** [Anthropic - Prompt Engineering Best Practices](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering)

### Phase 1: Analysis & Benchmarking
- [ ] **Analyze**: Identify where the current prompt fails or why it consumes too many tokens.
- [ ] **Baseline**: Record success rate and average token count.

### Phase 2: Rewriting & Compression
- [ ] **Clarity**: Remove unnecessary clauses, use clear imperative moods.
- [ ] **Compression**: Reduce token count without loss of meaning (Stopwords cleaning etc.).
- [ ] **Prompt Chaining**: Divide tasks into small, chained prompts instead of a single giant prompt.

### Phase 3: Verification (A/B Test)
- [ ] **Test**: Compare optimized prompt with the old one on same inputs.
- [ ] **Score**: Score based on response accuracy, speed and cost.

### Checkpoints
| Phase | Verification                                |
| ----- | ------------------------------------------- |
| 1     | Achieved >20% token savings?                |
| 2     | Did success rate (Accuracy) drop?           |
| 3     | Does model output format remain consistent? |
